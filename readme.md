# o-nlp

TBD ğŸš§

## Bugs ğŸ›

- [X] fix seed during model instantiation and add function
- [ ] approximate train/val loss logging for pretraining (see NanoGPT)
- [ ] T5 accelerate fp16bug? (see with bf16)

## Todo ğŸ¯

- [X] T5 implementations
- [X] T5 w/ LoRA fine-tuning
- [X] cpu training loop for debugging
- [ ] pretraining setup for fsbart, and others
- [ ] dynamic ngram masking with custom dataloader
- [ ] FIM autoregressive models
  - GLM
  - GPT-NeoX (pythia suite)
